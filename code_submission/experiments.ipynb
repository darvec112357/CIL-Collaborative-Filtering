{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from time import time\n",
    " \n",
    "from train_test import get_train_test, convert_df_to_matrix, _read_df_in_format\n",
    "\n",
    "# Surprise package for dataloading and evaluation\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import accuracy as acc    \n",
    "from surprise import KNNWithMeans, SVD, NMF, SlopeOne, BaselineOnly, NormalPredictor\n",
    "\n",
    "# Baseline 1: Averaging\n",
    "from averaging import UserAverage, ItemAverage, UserItemAverage\n",
    "\n",
    "# Baseline 2: Iterative SVD \n",
    "from svdals import normalize, ALS\n",
    "\n",
    "# Baseline 3: Neural Collaborative Filtering\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from ncf import CFDataset, GMF, MLP, NeuMF\n",
    "from ncf import nn_train, nn_predict\n",
    "\n",
    "# BFM + Adaptions \n",
    "from bfm import run_bfm, generate_clusters, run_bfm_augmented, get_RelationBlocks\n",
    "\n",
    "import myfm\n",
    "\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is structured as follows:\n",
    "1. Baseline: Simple averaging\n",
    "2. Baseline: Iterative SVD + ALS\n",
    "3. Baseline: NCF\n",
    "4. BFM + Simple Adaptations\n",
    "5. RainFM\n",
    "\n",
    "The data used are those provided within the Kaggle competition. To set the data folder, please refer to the **Data Loading** section and change the ```data_folder```.  \n",
    "To produce the exact Kaggle outputs, please refer to the **RainFM** section and change the ```TRAIN_MODE``` to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluationn\n",
    "def print_scores(y_true, y_pred, name):\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    print(f\"Method: {name}, RMSE: {rmse:.5f}, MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: These experiments are conducted on a custom train-test split for comparison. To generate the final results for submission, the data will just be the full training data set (train_df_full) instead. This can be adjusted via the ```TRAIN_MODE``` for our final method and submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/' # SET THIS TO YOUR DATA FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = get_train_test(os.path.join(data_folder, 'data_train.csv'), split_num=0)\n",
    "train_matrix = convert_df_to_matrix(train_df)\n",
    "\n",
    "y_true = test_df['Prediction'].to_numpy()\n",
    "\n",
    "train_df_full = _read_df_in_format(os.path.join(data_folder, 'data_train.csv'))\n",
    "train_matrix_full = convert_df_to_matrix(train_df_full)\n",
    "ans_df = _read_df_in_format(os.path.join(data_folder, 'sampleSubmission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For usage with Surprise package\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "train_data = Dataset.load_from_df(train_df, reader)\n",
    "test_data = Dataset.load_from_df(test_df, reader)\n",
    "\n",
    "trainset = train_data.build_full_trainset()\n",
    "testset = test_data.build_full_trainset().build_testset()\n",
    "anti_trainset = trainset.build_anti_testset()\n",
    "\n",
    "# Full trainset\n",
    "train_data_full = Dataset.load_from_df(train_df_full, reader)\n",
    "trainset_full = train_data_full.build_full_trainset()\n",
    "anti_trainset_full = trainset_full.build_anti_testset()\n",
    "\n",
    "# For final submission\n",
    "ans_data = Dataset.load_from_df(ans_df, reader)\n",
    "ansset = ans_data.build_full_trainset().build_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 1: Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is performed primarily as a sanity check for future methods; it represents the average across the various dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: UserAverage, RMSE: 1.0949, MAE: 0.9009, Time: 3.20\n",
      "Method: ItemAverage, RMSE: 1.0309, MAE: 0.8398, Time: 1.22\n",
      "Method: UserItemAverage, RMSE: 1.0314, MAE: 0.8482, Time: 2.17\n"
     ]
    }
   ],
   "source": [
    "methods = [UserAverage, ItemAverage, UserItemAverage]\n",
    "\n",
    "for method in methods:\n",
    "    start_time = time()\n",
    "    algo = method()\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    print(f\"Method: {method.__name__}, RMSE: {acc.rmse(predictions, False):.4f}, MAE: {acc.mae(predictions, False):.4f}, Time: {time() - start_time:.2f}\", end = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 2: SVD + ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method first applies SVD iteratively with shrinkage as an initialization for the U and V matrices. After which, the decomposition is limited to $k$ ranks and Alternating Least Squares is performed to optimize the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix_na = train_matrix.copy()\n",
    "train_matrix_na[train_matrix_na == 0] = np.nan\n",
    "A, mean, std = normalize(train_matrix_na)\n",
    "\n",
    "A = A.to_numpy()\n",
    "A[np.isnan(A)] = 0\n",
    "mask_A = A != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing IterSVD\n",
      "IterSVD completelete\n",
      "Iteration 1\t\tError after solving for U matrix: 0.8906752977216362\t\tError after solving for V matrix: 0.8708053729411698\n",
      "Iteration 2\t\tError after solving for U matrix: 0.8694372505362978\t\tError after solving for V matrix: 0.8636924463845832\n",
      "Iteration 3\t\tError after solving for U matrix: 0.8645094918264005\t\tError after solving for V matrix: 0.8616159457467563\n",
      "Iteration 4\t\tError after solving for U matrix: 0.862471854297844\t\tError after solving for V matrix: 0.8606922252739754\n",
      "Iteration 5\t\tError after solving for U matrix: 0.8614053721819654\t\tError after solving for V matrix: 0.8601915538561902\n",
      "Iteration 6\t\tError after solving for U matrix: 0.8607683438029089\t\tError after solving for V matrix: 0.8598855986892148\n",
      "Iteration 7\t\tError after solving for U matrix: 0.8603541840677057\t\tError after solving for V matrix: 0.8596830864651613\n",
      "Iteration 8\t\tError after solving for U matrix: 0.8600684730715922\t\tError after solving for V matrix: 0.859541208642487\n",
      "Iteration 9\t\tError after solving for U matrix: 0.8598624893482871\t\tError after solving for V matrix: 0.8594374960764972\n",
      "Iteration 10\t\tError after solving for U matrix: 0.8597088088427434\t\tError after solving for V matrix: 0.8593591340473629\n",
      "Iteration 11\t\tError after solving for U matrix: 0.8595909520495948\t\tError after solving for V matrix: 0.8592983346686083\n",
      "Iteration 12\t\tError after solving for U matrix: 0.8594984971142637\t\tError after solving for V matrix: 0.8592501220473456\n",
      "Iteration 13\t\tError after solving for U matrix: 0.8594245763036006\t\tError after solving for V matrix: 0.8592111856334798\n",
      "Iteration 14\t\tError after solving for U matrix: 0.8593645076630638\t\tError after solving for V matrix: 0.8591792479389893\n",
      "Iteration 15\t\tError after solving for U matrix: 0.8593150068071302\t\tError after solving for V matrix: 0.8591526974234533\n",
      "Iteration 16\t\tError after solving for U matrix: 0.8592737127089836\t\tError after solving for V matrix: 0.8591303659851407\n",
      "Iteration 17\t\tError after solving for U matrix: 0.8592388917489734\t\tError after solving for V matrix: 0.8591113891101938\n",
      "Iteration 18\t\tError after solving for U matrix: 0.8592092471225419\t\tError after solving for V matrix: 0.8590951151931354\n",
      "Iteration 19\t\tError after solving for U matrix: 0.8591837927087778\t\tError after solving for V matrix: 0.8590810451281593\n",
      "Iteration 20\t\tError after solving for U matrix: 0.8591617675773919\t\tError after solving for V matrix: 0.8590687910991592\n",
      "Time: 773.44\n"
     ]
    }
   ],
   "source": [
    "als = ALS()\n",
    "start = time()\n",
    "U, V = als.ALS(A, mask_A, k=3, shrinkage=30, lambd=0.1, n_iter_svd=5, n_iter_als=20)\n",
    "end = time(); print(f\"Time: {end - start:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = als.predict(U, V, mean, std)\n",
    "row_ids = test_df.row.to_numpy() - 1\n",
    "col_ids = test_df.col.to_numpy() - 1\n",
    "test_preds = predictions[row_ids, col_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Iterative SVD with ALS, RMSE: 0.9921, MAE: 0.7896\n"
     ]
    }
   ],
   "source": [
    "print_scores(y_true, test_preds, \"Iterative SVD with ALS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 3: NCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses Neural Collaborative Filtering. To improve the process, a Generalzied Factorization Machine and a Multi-Layer Perceptron are first trained separately, then used as pre-trained weights for the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CFDataset(train_df.values)\n",
    "train_loader = DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "\n",
    "testset = CFDataset(test_df.values)\n",
    "test_loader = DataLoader(testset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "hidden_dims = [64, 32]\n",
    "num_users, num_items = train_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMF\n",
    "Generalized Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Batch: 3600, Loss: 1.038\r"
     ]
    }
   ],
   "source": [
    "gmf = GMF(latent_dim=latent_dim, num_users=num_users, num_items=num_items)\n",
    "loss_function = MSELoss()\n",
    "optimizer = Adam(gmf.parameters(), lr=0.001)\n",
    "\n",
    "model = nn_train(gmf, train_loader, loss_function, optimizer)\n",
    "torch.save(model.state_dict(), 'models/gmf.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: GMF Only, RMSE: 1.0822, MAE: 0.8795\n"
     ]
    }
   ],
   "source": [
    "y_pred_gmf = nn_predict(gmf, test_loader)\n",
    "y_pred_gmf = np.clip(y_pred_gmf, 1, 5)\n",
    "print_scores(y_true, y_pred_gmf, \"GMF Only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Batch: 3600, Loss: 1.024\r"
     ]
    }
   ],
   "source": [
    "mlp = MLP(latent_dim=latent_dim, num_users=num_users, num_items=num_items, hidden_layers=hidden_dims)\n",
    "loss_function = MSELoss()\n",
    "optimizer = Adam(mlp.parameters(), lr=0.001)\n",
    "\n",
    "model = nn_train(mlp, train_loader, loss_function, optimizer)\n",
    "torch.save(model.state_dict(), 'models/mlp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: MLP Only, RMSE: 1.0029, MAE: 0.8105\n"
     ]
    }
   ],
   "source": [
    "y_pred_mlp = nn_predict(mlp, test_loader)\n",
    "y_pred_mlp = np.clip(y_pred_mlp, 1, 5)\n",
    "print_scores(y_true, y_pred_mlp, \"MLP Only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuMF with pre-training\n",
    "This makes use of the previously learnt GMF and MLP as initializations for the NeuMF model. The models are weighted by an $\\alpha$ value where $\\alpha=0$ fully uses the MLP model, while $\\alpha=1$ fully uses the GMF model.  \n",
    "After tuning for various $\\alpha$ values, an appropriate value was selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_pretrained = NeuMF(latent_dim=latent_dim, num_users=num_users, num_items=num_items, hidden_layers=hidden_dims, pretrained=True, alpha=0.05)\n",
    "neumf_dict = neumf_pretrained.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gmf_state_dict = torch.load('models/gmf.pth')\n",
    "mlp_state_dict = torch.load('models/mlp.pth')\n",
    "\n",
    "pretrained_dict_gmf = {k: v for k, v in gmf_state_dict.items() if k in neumf_dict}\n",
    "pretrained_dict_mlp = {k: v for k, v in mlp_state_dict.items() if k in neumf_dict}\n",
    "\n",
    "neumf_dict.update(pretrained_dict_gmf)\n",
    "neumf_dict.update(pretrained_dict_mlp)\n",
    "neumf_pretrained.load_state_dict(neumf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Batch: 3600, Loss: 0.997\r"
     ]
    }
   ],
   "source": [
    "loss_function = MSELoss()\n",
    "optimizer = SGD(neumf_pretrained.parameters(), lr=0.001)\n",
    "\n",
    "model = nn_train(neumf_pretrained, train_loader, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: NeuMF Pretrained, RMSE: 1.0041, MAE: 0.8092\n"
     ]
    }
   ],
   "source": [
    "y_pred_neumf_pretrained = nn_predict(neumf_pretrained, test_loader)\n",
    "y_pred_neumf_pretrained = np.clip(y_pred_neumf_pretrained, 1, 5)\n",
    "print_scores(y_true, y_pred_neumf_pretrained, \"NeuMF Pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "This only uses the individual ratings, without utilising any other knowledge about user/item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = 0.12, cutpoint = ['-1.983', '-1.237', '-0.263', '0.613'] : 100%|██████████| 200/200 [02:43<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred_bfm, fm = run_bfm(train_df, test_df, rank=10, fm_kind='classifier') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: BFM Baseline, RMSE: 0.9777, MAE: 0.7809\n"
     ]
    }
   ],
   "source": [
    "print_scores(y_true, y_pred_bfm, \"BFM Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusing with KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing preliminary experiments with various methods, we noticed that K-Nearest-Neighbours worked relatively well despite its simplicity. Therefore, we wanted to supplement the BFM with KNN predictions.  \n",
    "Using KNN to train the model, we generated predictions for all datapoints within rating matrix and their corresponding clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "antitrain_df = generate_clusters(trainset, anti_trainset, n_clusters=30)\n",
    "# antitrain_df.to_csv('models/knn_clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alpha = 2.49 w0 = 2.58 : 100%|██████████| 200/200 [04:47<00:00,  1.44s/it]\n",
      "alpha = 2.49 w0 = 2.58 : 100%|██████████| 200/200 [05:52<00:00,  1.76s/it]\n",
      "alpha = 2.49 w0 = 2.58 : 100%|██████████| 200/200 [05:14<00:00,  1.57s/it]\n",
      "alpha = 2.49 w0 = 2.58 : 100%|██████████| 200/200 [05:17<00:00,  1.59s/it]\n",
      "alpha = 2.49 w0 = 2.58 : 100%|██████████| 200/200 [07:21<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "y_pred_ensemble = run_bfm_augmented(train_df, antitrain_df, test_df, n_samples_per_cluster=50000, rank=10, seed_lst=[1, 42, 66, 88, 420])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: BFM with KNN Preds, RMSE: 0.9840, MAE: 0.7808\n"
     ]
    }
   ],
   "source": [
    "print_scores(y_true, y_pred_ensemble['Prediction_avg'], \"BFM with KNN Preds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion method on the full training set (to combine in the final blended model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "antitrain_df_full = generate_clusters(trainset_full, anti_trainset_full, n_clusters=30)\n",
    "# antitrain_df_full.to_csv('models/knn_clusters_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alpha = 2.20 w0 = 2.59 : 100%|██████████| 200/200 [06:26<00:00,  1.93s/it]\n",
      "alpha = 2.20 w0 = 2.59 : 100%|██████████| 200/200 [07:52<00:00,  2.36s/it]\n",
      "alpha = 2.20 w0 = 2.59 : 100%|██████████| 200/200 [07:15<00:00,  2.18s/it]\n",
      "alpha = 2.20 w0 = 2.59 : 100%|██████████| 200/200 [08:16<00:00,  2.48s/it]\n",
      "alpha = 2.20 w0 = 2.59 : 100%|██████████| 200/200 [08:46<00:00,  2.63s/it]\n"
     ]
    }
   ],
   "source": [
    "y_pred_ensemble_full = run_bfm_augmented(train_df_full, antitrain_df_full, ans_df, n_samples_per_cluster=50000, rank=10, seed_lst=[1, 42, 66, 88, 420])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>col</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction_1</th>\n",
       "      <th>Prediction_42</th>\n",
       "      <th>Prediction_66</th>\n",
       "      <th>Prediction_88</th>\n",
       "      <th>Prediction_420</th>\n",
       "      <th>Prediction_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.241306</td>\n",
       "      <td>3.274826</td>\n",
       "      <td>3.262135</td>\n",
       "      <td>3.280221</td>\n",
       "      <td>3.247981</td>\n",
       "      <td>3.261294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.082694</td>\n",
       "      <td>3.070482</td>\n",
       "      <td>3.084990</td>\n",
       "      <td>3.073493</td>\n",
       "      <td>3.095778</td>\n",
       "      <td>3.081487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.754485</td>\n",
       "      <td>3.742079</td>\n",
       "      <td>3.738020</td>\n",
       "      <td>3.753527</td>\n",
       "      <td>3.746164</td>\n",
       "      <td>3.746855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.305074</td>\n",
       "      <td>3.267090</td>\n",
       "      <td>3.296088</td>\n",
       "      <td>3.286407</td>\n",
       "      <td>3.310042</td>\n",
       "      <td>3.292940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.480581</td>\n",
       "      <td>3.446603</td>\n",
       "      <td>3.457871</td>\n",
       "      <td>3.476602</td>\n",
       "      <td>3.457095</td>\n",
       "      <td>3.463750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row  col  Prediction  Prediction_1  Prediction_42  Prediction_66  \\\n",
       "0   37    1           3      3.241306       3.274826       3.262135   \n",
       "1   73    1           3      3.082694       3.070482       3.084990   \n",
       "2  156    1           3      3.754485       3.742079       3.738020   \n",
       "3  160    1           3      3.305074       3.267090       3.296088   \n",
       "4  248    1           3      3.480581       3.446603       3.457871   \n",
       "\n",
       "   Prediction_88  Prediction_420  Prediction_avg  \n",
       "0       3.280221        3.247981        3.261294  \n",
       "1       3.073493        3.095778        3.081487  \n",
       "2       3.753527        3.746164        3.746855  \n",
       "3       3.286407        3.310042        3.292940  \n",
       "4       3.476602        3.457095        3.463750  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ensemble_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RainFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first section, we run various simple baseline models. Subsequently, we conducted Bayesian Factorization Machines augmented by various features. We then split the users into various clusters to allow us to have different models for each cluster. To produce the final model, we then blended models together for each cluster to produce ensembled predictions for submission.\n",
    "\n",
    "The results for each of the baseline methods and the final blended method are reported at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE = True # SET THIS TO FALSE IF YOU WANT TO TRAIN FINAL MODEL FOR SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODE:\n",
    "    train_df = train_df_full\n",
    "    test_df = ans_df\n",
    "    trainset = trainset_full\n",
    "    testset = ansset\n",
    "\n",
    "train_models = train_df\n",
    "test_models = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model):\n",
    "    \"\"\"\n",
    "    Given a trained model, returns the predicted ratings for the training and test sets.\n",
    "\n",
    "    Args:\n",
    "    - model: A trained model object that has a `test()` method.\n",
    "\n",
    "    Returns:\n",
    "    - train_pred: A list of predicted ratings for the training set.\n",
    "    - test_pred: A list of predicted ratings for the test set.\n",
    "    \"\"\"\n",
    "    train_predictions = model.test(trainset.build_testset())\n",
    "    test_predictions = model.test(testset)\n",
    "\n",
    "\n",
    "    mapping = {}\n",
    "    for prediciton in train_predictions:\n",
    "        mapping[(prediciton.uid, prediciton.iid)] = prediciton.est\n",
    "\n",
    "    train_pred = []\n",
    "    for index, row in train_df.iterrows():\n",
    "        train_pred.append(mapping[(row.row, row.col)])\n",
    "\n",
    "\n",
    "    mapping= {}\n",
    "    for prediciton in test_predictions:\n",
    "        mapping[(prediciton.uid, prediciton.iid)] = prediciton.est\n",
    "\n",
    "    test_pred = []\n",
    "    for index, row in test_df.iterrows():\n",
    "        test_pred.append(mapping[(row.row, row.col)])\n",
    "\n",
    "    return train_pred, test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnM = KNNWithMeans(k = 215, min_k= 11, verbose= False, sim_options = {\"name\": \"pearson_baseline\", \"user_based\": False})\n",
    "knnM.fit(trainset)\n",
    "\n",
    "train_predictions, test_predictions = get_predictions(knnM)\n",
    "train_models[\"KNNWithMeans\"] = train_predictions\n",
    "test_models[\"KNNWithMeans\"] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "baseline = BaselineOnly(bsl_options={'method': 'als', 'n_epochs': 155, 'reg_u': 12.389737311297987, 'reg_i':0.00010816431053605042})\n",
    "baseline.fit(trainset)\n",
    "\n",
    "train_predictions, test_predictions = get_predictions(baseline)\n",
    "train_models[\"BaselineOnly\"] = train_predictions\n",
    "test_models[\"BaselineOnly\"] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfm = NMF(n_factors =210, n_epochs = 185 ,reg_pu = 3.554509817040489 , reg_qi = 0.053153974679699414, biased = True)\n",
    "nfm.fit(trainset)\n",
    "\n",
    "train_predictions, test_predictions = get_predictions(nfm)\n",
    "train_models[\"NMF\"] = train_predictions\n",
    "test_models[\"NMF\"] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopeOne = SlopeOne()\n",
    "slopeOne.fit(trainset)\n",
    "\n",
    "train_predictions, test_predictions = get_predictions(slopeOne)\n",
    "train_models[\"SlopeOne\"] = train_predictions\n",
    "test_models[\"SlopeOne\"] = test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalPredictor = NormalPredictor()\n",
    "normalPredictor.fit(trainset)\n",
    "\n",
    "train_predictions, test_predictions = get_predictions(normalPredictor)\n",
    "train_models[\"NormalPredictor\"] = train_predictions\n",
    "test_models[\"NormalPredictor\"] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SvD = SVD(n_factors = 380, n_epochs = 115, lr_all = 0.003308328065201225, reg_all = 0.07818404435260741 )\n",
    "SvD.fit(trainset)\n",
    "\n",
    "train_predictions, test_predictions = get_predictions(SvD)\n",
    "train_models[\"SVD\"] = train_predictions\n",
    "test_models[\"SVD\"] = test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Factorization Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_blocks, test_blocks, feature_group_sizes = get_RelationBlocks(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = 0.10, cutpoint = ['-5.847', '-2.140', '-1.360', '-0.342', '0.577'] : 100%|██████████| 896/896 [3:50:20<00:00, 15.42s/it]    \n"
     ]
    }
   ],
   "source": [
    "bfm_OrderProbit_6 = myfm.MyFMOrderedProbit(rank=27)\n",
    "bfm_OrderProbit_6.fit(\n",
    "    None, train_df.Prediction.values, X_rel=train_blocks,\n",
    "    group_shapes=feature_group_sizes,\n",
    "    n_iter=896, n_kept_samples=895,\n",
    ");\n",
    "\n",
    "train_prediction = bfm_OrderProbit_6.predict_proba(None, train_blocks)\n",
    "train_models[\"bfm_OrderProbit_6\"] = train_prediction.dot(np.arange(6))\n",
    "\n",
    "test_prediction = bfm_OrderProbit_6.predict_proba(None, test_blocks)\n",
    "test_models[\"bfm_OrderProbit_6\"] = test_prediction.dot(np.arange(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = -0.00, cutpoint = ['-2.189', '-1.411', '-0.397', '0.523'] : 100%|██████████| 896/896 [2:17:03<00:00,  9.18s/it]    \n"
     ]
    }
   ],
   "source": [
    "bfm_OrderProbit_5 = myfm.MyFMOrderedProbit(rank=27)\n",
    "bfm_OrderProbit_5.fit(\n",
    "    None, train_df.Prediction - 1, X_rel=train_blocks,\n",
    "    group_shapes=feature_group_sizes,\n",
    "    n_iter=896, n_kept_samples=895,\n",
    ");\n",
    "\n",
    "train_prediction = bfm_OrderProbit_5.predict_proba(None, train_blocks)\n",
    "train_models[\"bfm_OrderProbit_5\"] = train_prediction.dot(np.arange(5)) + 1\n",
    "\n",
    "test_prediction = bfm_OrderProbit_5.predict_proba(None, test_blocks)\n",
    "test_models[\"bfm_OrderProbit_5\"] = test_prediction.dot(np.arange(5)) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = 0.01, cutpoint = ['-5.793', '-2.170', '-1.400', '-0.393', '0.519'] : 100%|██████████| 900/900 [19:30<00:00,  1.30s/it] \n"
     ]
    }
   ],
   "source": [
    "bfm_OrderProbit_6_small = myfm.MyFMOrderedProbit(rank=23)\n",
    "bfm_OrderProbit_6_small.fit(\n",
    "    None, train_df.Prediction , X_rel=train_blocks,\n",
    "    group_shapes=feature_group_sizes,\n",
    "    n_iter=900, n_kept_samples=900,\n",
    ");\n",
    "\n",
    "train_prediction = bfm_OrderProbit_6_small.predict_proba(None, train_blocks)\n",
    "train_models[\"bfm_OrderProbit_6_small\"] = train_prediction.dot(np.arange(6))\n",
    "\n",
    "test_prediction = bfm_OrderProbit_6_small.predict_proba(None, test_blocks)\n",
    "test_models[\"bfm_OrderProbit_6_small\"] = test_prediction.dot(np.arange(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = -0.02, cutpoint = ['-2.194', '-1.424', '-0.417', '0.492'] : 100%|██████████| 900/900 [18:04<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "bfm_OrderProbit_5_small = myfm.MyFMOrderedProbit(rank=23)\n",
    "bfm_OrderProbit_5_small.fit(\n",
    "    None, train_df.Prediction - 1, X_rel=train_blocks,\n",
    "    group_shapes=feature_group_sizes,\n",
    "    n_iter=900, n_kept_samples=900,\n",
    ");\n",
    "\n",
    "train_prediction = bfm_OrderProbit_5_small.predict_proba(None, train_blocks)\n",
    "train_models[\"bfm_OrderProbit_5_small\"] = train_prediction.dot(np.arange(5)) + 1\n",
    "\n",
    "test_prediction = bfm_OrderProbit_5_small.predict_proba(None, test_blocks)\n",
    "test_models[\"bfm_OrderProbit_5_small\"] = test_prediction.dot(np.arange(5)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alpha = 1.06 w0 = 3.30 : 100%|██████████| 701/701 [26:12<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "bfm_variational = myfm.VariationalFMRegressor(rank=29)\n",
    "bfm_variational .fit(\n",
    "    None, train_df.Prediction, X_rel=train_blocks,\n",
    "    group_shapes=feature_group_sizes,\n",
    "    n_iter=701\n",
    ");\n",
    "\n",
    "\n",
    "train_models[\"bfm_variational\"] = bfm_variational.predict(None,train_blocks)\n",
    "test_models[\"bfm_variational\"] = bfm_variational.predict(None,test_blocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models['avg_user_rating'] = train_df['row'].map(train_df.groupby('row')['Prediction'].mean())\n",
    "test_models['avg_user_rating'] = test_df['row'].map(train_df.groupby('row')['Prediction'].mean())\n",
    "\n",
    "train_models['avg_movie_rating'] = train_df['col'].map(train_df.groupby('col')['Prediction'].mean())\n",
    "test_models['avg_movie_rating'] = test_df['col'].map(train_df.groupby('col')['Prediction'].mean())\n",
    "\n",
    "\n",
    "train_models['user_rating_count'] = train_df['row'].map(train_df.groupby('row')['Prediction'].count())\n",
    "test_models['user_rating_count'] = test_df['row'].map(train_df.groupby('row')['Prediction'].count())\n",
    "\n",
    "train_models['movie_rating_count'] = train_df['col'].map(train_df.groupby('col')['Prediction'].count())\n",
    "test_models['movie_rating_count'] = test_df['col'].map(train_df.groupby('col')['Prediction'].count())\n",
    "\n",
    "train_models['user_rating_std'] = train_df['row'].map(train_df.groupby('row')['Prediction'].std())\n",
    "test_models['user_rating_std'] = test_df['row'].map(train_df.groupby('row')['Prediction'].std())\n",
    "\n",
    "train_models['movie_rating_std'] = train_df['col'].map(train_df.groupby('col')['Prediction'].std())\n",
    "test_models['movie_rating_std'] = test_df['col'].map(train_df.groupby('col')['Prediction'].std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the training data by user and count the number of ratings for each user\n",
    "user_count_train = train_df.groupby('row')['Prediction'].count()\n",
    "\n",
    "# Divide the users into four bins based on their rating count\n",
    "bin1 = user_count_train[user_count_train < user_count_train.quantile(0.25)].index\n",
    "bin2 = user_count_train[(user_count_train >= user_count_train.quantile(0.25)) &  (user_count_train < user_count_train.quantile(0.5))].index\n",
    "bin3 = user_count_train[(user_count_train >= user_count_train.quantile(0.5)) &  (user_count_train < user_count_train.quantile(0.75)) ].index\n",
    "bin4 = user_count_train[(user_count_train >= user_count_train.quantile(0.75))  ].index\n",
    "\n",
    "# Store the user indices for each bin in a list\n",
    "bins_train = [bin1.values, bin2.values, bin3.values, bin4.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElasticNet(alpha=0.1125, l1_ratio=0.055600000000000004, random_state=0,max_iter = 3869)\n",
    "model.fit(train_models[['bfm_OrderProbit_5','BaselineOnly','avg_user_rating','avg_movie_rating','NMF','bfm_variational','NormalPredictor']], train_df.Prediction)\n",
    "train_models[\"full_blend\"] = np.clip(model.predict(train_models[['bfm_OrderProbit_5','BaselineOnly','avg_user_rating','avg_movie_rating','NMF','bfm_variational','NormalPredictor']]),1,5)\n",
    "test_models[\"full_blend\"] = np.clip(model.predict(test_models[['bfm_OrderProbit_5','BaselineOnly','avg_user_rating','avg_movie_rating','NMF','bfm_variational','NormalPredictor']]),1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params =  [{'alpha': 0.1335, 'l1_ratio': 0.0001, 'max_iter': 8592},{'alpha': 0.1043, 'l1_ratio': 0.0734, 'max_iter': 4203},\n",
    "{'alpha': 0.10070000000000001, 'l1_ratio': 0.0629, 'max_iter': 8133},{'alpha': 0.1019, 'l1_ratio': 0.0007, 'max_iter': 4191}]\n",
    "usedIndexes = [['bfm_OrderProbit_5', 'bfm_variational', 'SlopeOne', 'avg_user_rating', 'user_rating_count', 'movie_rating_count', 'avg_movie_rating'],\n",
    " ['bfm_OrderProbit_5', 'bfm_variational', 'user_rating_count', 'movie_rating_count', 'NormalPredictor', 'BaselineOnly', 'avg_movie_rating'],\n",
    " ['bfm_OrderProbit_5', 'bfm_variational', 'user_rating_count', 'avg_movie_rating', 'movie_rating_count', 'BaselineOnly'],\n",
    " ['bfm_OrderProbit_5', 'bfm_variational', 'avg_movie_rating', 'NormalPredictor']]\n",
    "for i, bi in enumerate(bins_train):\n",
    "    params = best_params[i]\n",
    "    usedIndex = usedIndexes[i]\n",
    "    model = ElasticNet(**params)\n",
    "    model.fit(train_models[usedIndex],train_df.Prediction)\n",
    "    train_models.loc[train_models.row.isin(bi),\"grouping_blend\"] = np.clip(model.predict(train_models[train_models.row.isin(bi)][usedIndex]),1,5)\n",
    "    test_models.loc[test_models.row.isin(bi),\"grouping_blend\"] = np.clip(model.predict(test_models[test_models.row.isin(bi)][usedIndex]),1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Blend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCombined = ElasticNet(**{'alpha': 0.15504731878406247, 'l1_ratio': 0.15642, 'max_iter': 10000})\n",
    "modelCombined.fit(train_models[[\"full_blend\",\"grouping_blend\"]],train_df.Prediction)\n",
    "test_models[\"final_blend\"] = np.clip(modelCombined.predict(test_models[[\"full_blend\",\"grouping_blend\"]]),1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results / Submission\n",
    "If ```TRAIN_MODE``` is set to True, it will produce results based on our train-test split.  \n",
    "If ```TRAIN_MODE``` is set to False, it will produce results for Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: bfm_OrderProbit_6, RMSE: 0.97022, MAE: 0.7706\n",
      "Method: bfm_OrderProbit_5, RMSE: 0.97013, MAE: 0.7704\n",
      "Method: bfm_OrderProbit_6_small, RMSE: 0.97004, MAE: 0.7704\n",
      "Method: bfm_OrderProbit_5_small, RMSE: 0.97018, MAE: 0.7707\n",
      "Method: bfm_variational, RMSE: 0.97753, MAE: 0.7822\n",
      "Method: KNNWithMeans, RMSE: 0.99032, MAE: 0.7925\n",
      "Method: BaselineOnly, RMSE: 0.99900, MAE: 0.8047\n",
      "Method: SlopeOne, RMSE: 0.99965, MAE: 0.8017\n",
      "Method: NormalPredictor, RMSE: 1.48055, MAE: 1.1766\n",
      "Method: SVD, RMSE: 0.98621, MAE: 0.7958\n",
      "Method: avg_user_rating, RMSE: 1.09488, MAE: 0.9009\n",
      "Method: avg_movie_rating, RMSE: 1.03095, MAE: 0.8398\n",
      "Method: user_rating_count, RMSE: 136.13920, MAE: 120.8537\n",
      "Method: movie_rating_count, RMSE: 1778.72522, MAE: 1553.7553\n",
      "Method: user_rating_std, RMSE: 3.00811, MAE: 2.7948\n",
      "Method: movie_rating_std, RMSE: 3.06818, MAE: 2.8477\n",
      "Method: NMF, RMSE: 1.00298, MAE: 0.8123\n",
      "Method: full_blend, RMSE: 0.96956, MAE: 0.7707\n",
      "Method: grouping_blend, RMSE: 0.96951, MAE: 0.7690\n",
      "Method: final_blend, RMSE: 0.96948, MAE: 0.7714\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODE:\n",
    "    for col in test_models.columns[3:]:\n",
    "        print_scores(y_true, test_models[col], col)\n",
    "else:\n",
    "    submission_format = pd.read_csv(os.path.join(data_folder, 'sampleSubmission.csv'))\n",
    "    submission_format['Prediction'] = test_models['final_blend']\n",
    "    submission_format.to_csv(os.path.join(data_folder, 'final_submission.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
